{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -qU langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "     \n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Model Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-community langchain-openai langchain_experimental neo4j\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"12345678\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "     \n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph QA using GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  --quiet langchain langchain-openai langchain-community neo4j\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"12345678\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "     \n",
    "\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"), # gpt-4o-mini\tgpt-3.5-turbo\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\"),\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iprincetech/Desktop/Question-Answering-System-for-Raster-Data/.venv/lib/python3.13/site-packages/gradio/components/chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7889\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7889/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import gradio as gr\n",
    "# Defining questions\n",
    "questions = {\n",
    "    \"question_1\": \"What is the sensor type of Sentinel-2 L2A Maja Products\",\n",
    "    \"question_2\": \"What is the spatial extent of Sentinel-2 L2A Maja Products?\",\n",
    "    \"question_3\": \"What is the spatial extent of Sentinel-1 Floodmasks (Data4Human)?\",\n",
    "    \"question_4\": \"Give me the download links for Sentinel-2 L2A Maja Products?\",\n",
    "    \"question_5\": \"What is the spatial extent of S5P Tropomi L4 P1D NO2surf?\",\n",
    "    \"question_6\": \"What is the reference system used for Sentinel-2 L2A Maja Products?\",\n",
    "    \"question_7\": \"What is the ISO standards for Sentinel-2 L2A Maja Products?\",\n",
    "    \"question_8\": \"what is the temporal extent of S5P Tropomi L3 P1D CF?\",\n",
    "    \"question_9\": \"what is the orbit type of Sentinel-2 L2A Maja Products?\",\n",
    "    \"question_10\": \"provide the geojson of Sentinel-2 L2A Maja Products?\",\n",
    "    \"question_11\": \"provide the geojson of Global WaterPack - MODIS?\",\n",
    "    \"question_12\": \"what is the visualisation link of EnMAP L2A HSI Products?\",\n",
    "    \"question_13\": \"What id the sensor type of S5P Tropomi L3 P1D O3?\",\n",
    "    \"question_14\": \"what is the visualization link of Sentinel-2 L3A Monthly WASP Products?\",\n",
    "    \n",
    "   \n",
    "}\n",
    "\n",
    "def get_answer(messages):\n",
    "    question = messages[-1][0] \n",
    "    question_key = next((key for key, value in questions.items() if value.lower() == question.lower()), None)\n",
    "    try:\n",
    "        if question_key:\n",
    "            response = chain.invoke(questions[question_key])\n",
    "        else:\n",
    "            response = chain.invoke(question)\n",
    "        \n",
    "        if isinstance(response, dict) and 'result' in response:\n",
    "            return messages[:-1] + [[question, response['result'].strip()]] \n",
    "        else:\n",
    "            return messages[:-1] + [[question, \"No result found or invalid response format.\"]]\n",
    "    except Exception as e:\n",
    "        return messages[:-1] + [[question, f\"An error occurred while processing your question: {str(e)}\"]]\n",
    "\n",
    "def print_like_dislike(data: gr.LikeData):\n",
    "    print(f\"Message index: {data.index}, Liked: {data.liked}, Content: {data.value}\")\n",
    "\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"# STAChat\")\n",
    "    chatbot = gr.Chatbot(label=\"Chat History\", avatar_images=(\n",
    "            None,\n",
    "            \"bot.png\",))\n",
    "    user_input = gr.Textbox(placeholder=\"Ask your question here...\", label=\"Your Question\")\n",
    "    submit_button = gr.Button(\"Send\")\n",
    "    \n",
    "    def chat_interaction(messages, user_query):\n",
    "        if user_query:\n",
    "            messages = messages + [[user_query, None]]\n",
    "            return get_answer(messages), \"\"  \n",
    "        return messages, \"\"\n",
    "\n",
    "    chat_msg = user_input.submit(chat_interaction, [chatbot, user_input], [chatbot, user_input])\n",
    "    chat_msg.then(lambda: gr.Textbox(interactive=True), None, [user_input])\n",
    "\n",
    "    chatbot.like(print_like_dislike, None, None, like_user_message=True)\n",
    "\n",
    "    submit_button.click(\n",
    "        chat_interaction, \n",
    "        inputs=[chatbot, user_input], \n",
    "        outputs=[chatbot, user_input]\n",
    "    )\n",
    "    user_input.submit(\n",
    "        chat_interaction, \n",
    "        inputs=[chatbot, user_input], \n",
    "        outputs=[chatbot, user_input]\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iface.launch()\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
